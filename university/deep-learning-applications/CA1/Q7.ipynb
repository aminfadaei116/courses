{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvufRvT7DjXFGCuza/1pqH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminfadaei116/Deep-Learning-Course/blob/master/CA1/Q7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBPdTj1ADrow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09f52f20-56a3-4a6a-c57b-66e25e40593a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZNBGmm_M8GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOYJbulZM9gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test_data = pd.read_csv('/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/Reduced_Test_Data.csv', header = None)\n",
        "Test_label = pd.read_csv('/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/testLabels.csv', header = None)\n",
        "Train_data = pd.read_csv('/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/Reduced_Train_Data.csv', header = None)\n",
        "Train_label = pd.read_csv('/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/trainLabels.csv', header = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9hwKnOAM_6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test_data = Test_data.to_numpy()\n",
        "Test_label = Test_label.to_numpy()\n",
        "Train_data = Train_data.to_numpy()\n",
        "Train_label = Train_label.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q7SNIAeNCba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxD = np.max(Train_data, axis=0)\n",
        "minD = np.min(Train_data, axis=0)\n",
        "delta = maxD - minD\n",
        "Train_data = Train_data - minD\n",
        "Test_data = Test_data - minD\n",
        "Train_data = 2 * (Train_data/delta) - 1\n",
        "Test_data = 2 * (Test_data/delta) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFHImKlwcmfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num = 200\n",
        "noise_data = np.random.normal(loc=0, scale=0.2, size=(num, len(Train_data[0])))\n",
        "noise_label = 10 * np.ones(shape=(num,1))\n",
        "Train_data = np.concatenate((Train_data, noise_data))\n",
        "Train_label = np.concatenate((Train_label, noise_label))\n",
        "Train_data, Train_label =shuffle(Train_data, Train_label, random_state=0)\n",
        "Train_label = (Train_label).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPtK318rND8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(x):\n",
        "  return np.maximum(x, np.zeros_like(x))\n",
        "\n",
        "def Cal_accuracy(True_label, Estimated_label):\n",
        "  return np.count_nonzero(np.transpose(True_label)==np.array(Estimated_label))/True_label.size\n",
        "\n",
        "def step(x):\n",
        "  return 1*(x > np.zeros_like(x))\n",
        "\n",
        "def encode(vect, idx, landa):\n",
        "  temp = np.multiply(step(landa - vect),-1)\n",
        "  if idx != 10:\n",
        "    temp[idx] = 1\n",
        "  return temp\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * np.multiply(x, 1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
        "\n",
        "def Dgelu(x):\n",
        "  return np.divide(0.053516 * np.power(x, 3) + np.multiply(0.398942, x) , np.power(np.cosh(0.0356774 * np.power(x, 3) +np.multiply(0.79788, x)),2)) + 0.5 * ( 1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
        "\n",
        "def RBF_encode(data, number_class, dim):\n",
        "  data = np.reshape(data, (dim,int(data.size/dim)), order='F')\n",
        "  data = LA.norm(data,1, axis = 0)\n",
        "  data = np.reshape(data, (number_class, int(data.size/number_class)), order='F')\n",
        "  return data\n",
        "\n",
        "def expand(x):\n",
        "  temp = []\n",
        "  for i in range(len(x)):\n",
        "    temp.append(x[i])\n",
        "    temp.append(x[i])\n",
        "  return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9F88ZqANFWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network:\n",
        "  def __init__(self, input_node, hidden_layer, hidden_node, output_node):\n",
        "    self.number_input_node = input_node\n",
        "    self.number_hidden_layer = hidden_layer\n",
        "    self.number_hidden_node = hidden_node\n",
        "    self.number_class_output = output_node\n",
        "    self.distance_dim = 2\n",
        "    self.weight = {}\n",
        "    self.layer = {}\n",
        "    self.batch_data = {}\n",
        "    self.batch_label = {}\n",
        "    self.diff_weight = {}\n",
        "    self.score = []\n",
        "    self.result = []\n",
        "    self.init_weight()\n",
        "    self.loss = []\n",
        "    self.back_prob_output = []\n",
        "    for i in range(self.number_hidden_layer + 1):\n",
        "      self.diff_weight[\"w\" + str(i) + \"_prev\"] = 0\n",
        "      self.diff_weight[\"b\" + str(i) + \"_prev\"] = 0\n",
        "    self.momentum = 0.9\n",
        "    self.learning_rate = 0.001\n",
        "    self.number_turn = 100\n",
        "    self.batchS = 128\n",
        "    self.accuracy_graph = []\n",
        "    self.loss_graph = []\n",
        "    self.landa = 550\n",
        "    self.attack_rate = 0.1\n",
        "\n",
        "  def set_turn(self, x):\n",
        "    self.number_turn = x\n",
        "\n",
        "  def set_learning_rate(self, x):\n",
        "    self.learning_rate = x\n",
        "\n",
        "  def set_batch_size(self, x):\n",
        "    self.batchS = x\n",
        "  \n",
        "  def load_weights(self):\n",
        "    for i in range(self.number_hidden_layer + 1):\n",
        "      self.weight[\"w\" + str(i)] = (pd.read_csv(\"/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/w\" + str(i) + \".csv\", header=None)).to_numpy()\n",
        "      self.weight[\"b\" + str(i)] = (pd.read_csv(\"/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/b\" + str(i) + \".csv\", header=None)).to_numpy()\n",
        "\n",
        "  def init_weight(self):\n",
        "    self.weight[\"w0\"] = np.random.normal(loc=0, scale=0.14, size=(self.number_hidden_node[0], self.number_input_node))\n",
        "    self.weight[\"b0\"] = np.random.normal(loc=0, scale=0.14, size=(self.number_hidden_node[0], 1))\n",
        "    for i in range(self.number_hidden_layer - 1):\n",
        "      self.weight[\"w\" + str(i+1)] = (np.random.normal(loc=0, scale=0.14, size=(self.number_hidden_node[i+1], self.number_hidden_node[i])))\n",
        "      self.weight[\"b\" + str(i+1)] = (np.random.normal(loc=0, scale=0.14, size=(self.number_hidden_node[i+1], 1)))\n",
        "    self.weight[\"w\" + str(self.number_hidden_layer)] = np.random.normal(loc=0, scale=0.14, size=(self.number_class_output * self.distance_dim, self.number_hidden_node[self.number_hidden_layer-1]))\n",
        "    self.weight[\"b\" + str(self.number_hidden_layer)] = np.random.normal(loc=0, scale=0.14, size=(self.number_class_output * self.distance_dim, 1))\n",
        "    \n",
        "  def forward(self, data):\n",
        "    self.layer.clear()\n",
        "    self.layer[\"input\"] = data\n",
        "    if len(data) == data.size:\n",
        "      self.layer[\"z0\"] = (np.matmul(self.weight[\"w0\"],np.transpose(data))).reshape((len(MyNetwork.weight[\"w0\"]),1))\n",
        "    else:\n",
        "      self.layer[\"z0\"] = (np.matmul(self.weight[\"w0\"], np.transpose(data)))\n",
        "    self.layer[\"z0\"] = np.add(self.layer[\"z0\"], self.weight[\"b0\"])\n",
        "    self.layer[\"r0\"] = gelu(self.layer[\"z0\"])\n",
        "    self.layer[\"s0\"] = Dgelu(self.layer[\"z0\"])\n",
        "    for i in range(self.number_hidden_layer - 1):\n",
        "      self.layer[\"z\" + str(i + 1)] = np.matmul(self.weight[\"w\" + str(i + 1)], self.layer[\"r\" + str(i)])\n",
        "      self.layer[\"z\" + str(i + 1)] = np.add(self.layer[\"z\" + str(i + 1)], self.weight[\"b\" + str(i + 1)])\n",
        "      self.layer[\"r\" + str(i + 1)] = gelu(self.layer[\"z\" + str(i + 1)])\n",
        "      self.layer[\"s\" + str(i + 1)] = Dgelu(self.layer[\"z\" + str(i + 1)])\n",
        "    self.layer[\"z\" + str(self.number_hidden_layer)] = np.matmul(self.weight[\"w\" + str(self.number_hidden_layer)], self.layer[\"r\" + str(self.number_hidden_layer - 1)])\n",
        "    self.layer[\"z\" + str(self.number_hidden_layer)] = np.add(self.layer[\"z\" + str(self.number_hidden_layer)], self.weight[\"b\" + str(self.number_hidden_layer)])\n",
        "    self.layer[\"r\" + str(self.number_hidden_layer)] = RBF_encode(self.layer[\"z\" + str(self.number_hidden_layer)], self.number_class_output, self.distance_dim)\n",
        "    self.layer[\"s\" + str(self.number_hidden_layer)] = np.sign(self.layer[\"z\" + str(self.number_hidden_layer)])\n",
        "    self.score = self.layer[\"r\" + str(self.number_hidden_layer)]\n",
        "    #print(np.transpose(self.score))\n",
        "    temp = self.decide_RBF(self.score, self.landa)\n",
        "    return temp\n",
        "  \n",
        "  def decide_RBF(self, data, landa):\n",
        "    self.result = np.argmin(data, axis=0)\n",
        "    self.result[np.min(data, axis=0)>landa] = 10\n",
        "    return self.result\n",
        "\n",
        "  def cal_loss(self, label):\n",
        "    self.loss.clear()\n",
        "    self.back_prob_output = []\n",
        "    temp = np.transpose(self.score)\n",
        "    for i in range(len(label)):\n",
        "      if label[i] != 10:\n",
        "        self.loss.append(np.sum(relu((self.landa - temp[i]))) - relu(self.landa- temp[i][label[i]]) +   temp[i][int(label[i])])\n",
        "      else:\n",
        "        self.loss.append(np.sum(relu(self.landa - temp[i])))\n",
        "      self.back_prob_output.append(encode(temp[i], label[i], self.landa))\n",
        "    self.back_prob_output = np.transpose(self.back_prob_output)/len(label)\n",
        "    total_loss = np.sum(self.loss)/len(self.loss)\n",
        "    self.loss_graph.append(total_loss)\n",
        "    return total_loss\n",
        "\n",
        "  def batching(self, data, label):\n",
        "    batch_size = self.batchS\n",
        "    if len(data) == data.size:\n",
        "      batch[0] = data\n",
        "      return\n",
        "    for i in range(int(len(data)/batch_size)):\n",
        "      self.batch_data[i] = data[i*batch_size:(i+1)*batch_size]\n",
        "      self.batch_label[i] = label[i*batch_size:(i+1)*batch_size]\n",
        "    if len(data)%batch_size != 0:\n",
        "      self.batch_data[int(len(data)/batch_size)] = data[int(len(data)/batch_size)*batch_size:len(data)]\n",
        "      self.batch_label[int(len(data)/batch_size)] = label[int(len(data)/batch_size)*batch_size:len(data)]\n",
        "\n",
        "  def back_prob(self):\n",
        "    temp = np.multiply(expand(self.back_prob_output), self.layer[\"s\" + str(self.number_hidden_layer)])\n",
        "    for i in range(self.number_hidden_layer):\n",
        "      self.diff_weight[\"w\" + str(self.number_hidden_layer - i)] = np.matmul(temp, np.transpose(self.layer[\"r\"+ str(self.number_hidden_layer-1-i)]))\n",
        "      self.diff_weight[\"b\" + str(self.number_hidden_layer - i)] = np.matmul(temp, np.ones((len(temp[0]), 1)))\n",
        "      temp = np.matmul(np.transpose(self.weight[\"w\" + str(self.number_hidden_layer - i)]), temp)\n",
        "      temp = np.multiply(self.layer[\"s\" + str(self.number_hidden_layer - i - 1)], temp)\n",
        "    self.diff_weight[\"w0\"] = np.matmul(temp, self.layer[\"input\"])\n",
        "    self.diff_weight[\"b0\"] = np.matmul(temp, np.ones((len(temp[0]), 1)))\n",
        "\n",
        "    for i in range(self.number_hidden_layer + 1):\n",
        "      self.diff_weight[\"w\" + str(i) + \"_prev\"] = ((1 - self.momentum) * self.diff_weight[\"w\" + str(i)] + self.momentum * self.diff_weight[\"w\" + str(i) + \"_prev\"])\n",
        "      self.diff_weight[\"b\" + str(i) + \"_prev\"] = ((1 - self.momentum) * self.diff_weight[\"b\" + str(i)] + self.momentum * self.diff_weight[\"b\" + str(i) + \"_prev\"])\n",
        "\n",
        "    for i in range(self.number_hidden_layer + 1):\n",
        "      self.weight[\"w\" + str(i)] = self.weight[\"w\" + str(i)] - self.diff_weight[\"w\" + str(i) + \"_prev\"] * self.learning_rate\n",
        "      self.weight[\"b\" + str(i)] = self.weight[\"b\" + str(i)] - self.diff_weight[\"b\" + str(i) + \"_prev\"] * self.learning_rate\n",
        "\n",
        "  def Train_Network(self, data, label):\n",
        "    epoch = 0\n",
        "    for j in range(self.number_turn):\n",
        "      self.batching(data, label)\n",
        "      for i in range(len(self.batch_data)):\n",
        "        self.forward(self.batch_data[i])\n",
        "        self.accuracy_graph.append(Cal_accuracy(self.batch_label[i], self.forward(self.batch_data[i])))\n",
        "        print(self.cal_loss(self.batch_label[i]), \" step \", i,\" of the epoch \", j , \"the steps is: \", epoch)\n",
        "        self.back_prob()\n",
        "        epoch += 1\n",
        "\n",
        "  def Test_Network(self, data, label):\n",
        "    if(len(data) == data.size):\n",
        "      temp = self.batchS\n",
        "      self.batchS = 1\n",
        "      print(Cal_accuracy(label, MyNetwork.forward(data)))\n",
        "      self.batchS = temp\n",
        "    else:\n",
        "      print(Cal_accuracy(label, MyNetwork.forward(data)))\n",
        "\n",
        "  def attack_network(self, data, label):\n",
        "    self.forward(data)\n",
        "    self.cal_loss(label)\n",
        "    D_image = np.multiply(expand(self.back_prob_output), self.layer[\"s\" + str(self.number_hidden_layer)])\n",
        "    for i in range(self.number_hidden_layer):\n",
        "      D_image = np.matmul(np.transpose(self.weight[\"w\" + str(self.number_hidden_layer - i)]), D_image)\n",
        "      D_image = np.multiply(self.layer[\"s\" + str(self.number_hidden_layer - i - 1)], D_image)\n",
        "    D_image = np.matmul(np.transpose(self.weight[\"w0\"]), D_image)\n",
        "    D_image = np.sign(D_image) \n",
        "    return data - self.attack_rate * D_image.reshape((-1,))\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayu7NVd2eLR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MyNetwork = Network(128,1,[150] ,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5dwrDu_iKmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MyNetwork.load_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmDT1CDUeL8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MyNetwork.set_learning_rate(0.05)\n",
        "MyNetwork.set_turn(3)\n",
        "MyNetwork.set_batch_size(128)\n",
        "MyNetwork.Train_Network(Train_data, Train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnubnE3pj0n7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mini_dataSet, mini_labelSet = [],[]\n",
        "j = 0\n",
        "for i in range(1000):\n",
        "  if MyNetwork.forward(Train_data[i]) == Train_label[i]:\n",
        "    mini_dataSet.append(Train_data[i])\n",
        "    mini_labelSet.append(Train_label[i])\n",
        "    j += 1\n",
        "  if j == 100:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjuMvWyCwbDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attacked_image = []\n",
        "for i in range(100):\n",
        "  attacked_image.append(MyNetwork.attack_network(mini_dataSet[i], mini_labelSet[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc47PCDCnZSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score1 = 0\n",
        "score2 = 0\n",
        "for i in range(100):\n",
        "  print(\"The label for the\", i,\"th data is: \", MyNetwork.forward(mini_dataSet[i]), \"after attack it became: \", MyNetwork.forward(attacked_image[i].reshape((-1,))))\n",
        "  if MyNetwork.forward(mini_dataSet[i]) != MyNetwork.forward(attacked_image[i].reshape((-1,))):\n",
        "    score1 += 1\n",
        "  if 10 ==  MyNetwork.forward(attacked_image[i].reshape((-1,))):\n",
        "    score2 += 1\n",
        "print(\"The attacked accuracy was: \", score1)\n",
        "print(\"The number rejection is: \", score2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0Hmlx8QwoBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "62995d32-5dad-4aca-d32a-ba74fafb13da"
      },
      "source": [
        "for i in range(20):\n",
        "  print(\"The label for the\", i,\"th data is: \", MyNetwork.forward(Test_data[i]), \"after attack it became: \", MyNetwork.forward(attacked_image[i].reshape((-1,))), \"The true Label\", Test_label[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The label for the 0 th data is:  [9] after attack it became:  [10] The true Label [9]\n",
            "The label for the 1 th data is:  [2] after attack it became:  [2] The true Label [2]\n",
            "The label for the 2 th data is:  [1] after attack it became:  [9] The true Label [1]\n",
            "The label for the 3 th data is:  [1] after attack it became:  [3] The true Label [1]\n",
            "The label for the 4 th data is:  [6] after attack it became:  [3] The true Label [6]\n",
            "The label for the 5 th data is:  [1] after attack it became:  [10] The true Label [1]\n",
            "The label for the 6 th data is:  [4] after attack it became:  [5] The true Label [4]\n",
            "The label for the 7 th data is:  [6] after attack it became:  [6] The true Label [6]\n",
            "The label for the 8 th data is:  [5] after attack it became:  [10] The true Label [5]\n",
            "The label for the 9 th data is:  [7] after attack it became:  [10] The true Label [7]\n",
            "The label for the 10 th data is:  [2] after attack it became:  [8] The true Label [4]\n",
            "The label for the 11 th data is:  [7] after attack it became:  [10] The true Label [5]\n",
            "The label for the 12 th data is:  [5] after attack it became:  [10] The true Label [7]\n",
            "The label for the 13 th data is:  [3] after attack it became:  [0] The true Label [3]\n",
            "The label for the 14 th data is:  [4] after attack it became:  [10] The true Label [4]\n",
            "The label for the 15 th data is:  [1] after attack it became:  [10] The true Label [1]\n",
            "The label for the 16 th data is:  [2] after attack it became:  [8] The true Label [2]\n",
            "The label for the 17 th data is:  [4] after attack it became:  [8] The true Label [4]\n",
            "The label for the 18 th data is:  [8] after attack it became:  [10] The true Label [8]\n",
            "The label for the 19 th data is:  [0] after attack it became:  [10] The true Label [0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in cosh\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in power\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJNwvwdZlXdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97a5a290-5863-409e-87a8-001a235d5934"
      },
      "source": [
        "MyNetwork.Test_Network(Train_data, Train_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7976744186046512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi_JrlYvlSkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "58e05f8b-3dd7-450f-ea79-f176445af1c1"
      },
      "source": [
        "for i in range(MyNetwork.number_hidden_layer + 1):\n",
        "  pd.DataFrame(MyNetwork.weight[\"w\" + str(i)]).to_csv(\"/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/w\" + str(i) + \".csv\", index=False, header=False)\n",
        "  pd.DataFrame(MyNetwork.weight[\"b\" + str(i)]).to_csv(\"/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/b\" + str(i) + \".csv\", index=False, header=False)\n",
        "  pd.DataFrame(MyNetwork.accuracy_graph).to_csv(\"/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/accuracy.csv\", index=False, header=False)\n",
        "  pd.DataFrame(MyNetwork.loss_graph).to_csv(\"/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/loss.csv\", index=False, header=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-caadd85e0db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/b\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/accuracy.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Deep Learning Course Spring 99/DataSets/Fashion-MNIST/loss.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# last ditch effort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# last ditch effort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
          ]
        }
      ]
    }
  ]
}